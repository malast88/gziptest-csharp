# Abstract
This is a simple application which compresses and decompresses files using System.IO.Compression.GzipStream in multi-threaded manner (based on Thread and Monitor classes) with ability to process very big files which are could not be placed into RAM. Key principles used:
- GZIP file format allows it to be a series of "members" (compressed data sets) (RFC1952 https://tools.ietf.org/html/rfc1952)
- The Producer-Consumer pattern (https://ru.stackoverflow.com/questions/428327/%D0%98%D0%BC%D0%BF%D0%BB%D0%B5%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%86%D0%B8%D1%8F-producer-consumer-pattern)
This is a study project which is intended to expose my ability to implement multi-threaded code without using TPL.

# GZIP parallel compression/decompression
## Main idea
Since GZIP file is a series of compressed data sets, it is possible to split file to be compressed to blocks, then compress them independently in the separate threads using System.IO.Compression.GzipStream (which produces the correct GZIP data set) and finally write resulting compressed data into destination file. The reverse process consists of reading those data sets from the compressed file according to GZIP data fromat specified in RFC1952, decompress them independentely and compose the destination file.
## The importance of maintaining original order of blocks
It is obvious that both compression and decompression processes must retain the order of blocks read from source file when composing the result.
## Reading of compressed file
This naive implementation threats the two-bytes sequence 0x1f 0x8b as a start of compressed set.

# The architecture of the application
## Main idea
Both compression and decompression could be broken down into the following processes:
- Reading source file
- Compression
- Writing destination file
Reading and writing files could be done only sequentally, while compression could be done in parallel.
There is an established practice to organize a communcation between one reader/writer thread and multiple processing threads using the Producer-Consumer pattern, so it is a basis of the application's architecture.
## Producer-consumer chains
### Reader -> processors
The reader thread reads the input file sequentally and produces the blocks of data to compress/decompress. A number of processor threads consume them to perform compression or decompression.
Since the reader could produce the data very fast, which could lead to RAM overrun, the capacity of this chain should be restricted according to the number of processing threads.
Each block of data generated by reader must contain its sequence number in the input file in order to retain the order in the output file.
### Processors -> Writer
Processing threads produce data in random order, but writer thread must consume in the order from inpput file. Hence this chain must always provide the blocks of data in the proper order. It should block writer if the following block is not ready yet, even if there are next blocks available.
Because it is too hard (personally for me) to implement the proper loginc in the single producer-consumer queue, I've decided to implement one producer-consumer queue for each processing thread producing data. Each this queue provide the data in the order of ascending ids. Writer itself stores the last written id and examines those queues for the next.
### Reader - writer
This producer-consumer chain is very simple and is intended to pass the last id read from the input file to writer. Writer ends its job when the last written id equals to this id passed from reader.
### Total
There are 2+N producer-consumer chains, where N is the number of processing threads. In this scheme processing threads are intended to be both producers and consumers.
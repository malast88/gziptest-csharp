# Abstract
This is a simple application which compresses and decompresses files using System.IO.Compression.GzipStream in multi-threaded manner (based on Thread and Monitor classes) with ability to process very big files which are could not be placed into RAM. Key principles used:
- GZIP file format allows it to be a series of "members" (compressed data sets) (RFC1952 https://tools.ietf.org/html/rfc1952)
- The Producer-Consumer pattern (https://ru.stackoverflow.com/questions/428327/%D0%98%D0%BC%D0%BF%D0%BB%D0%B5%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%86%D0%B8%D1%8F-producer-consumer-pattern)

This is a **study** project which is intended to expose my ability to implement multi-threaded code in TDD manner without using TPL.

_Important note_:  This code does not act as a RFC-compliant decompressor while decompressing gzip file with multiple members inside. Since it is a study project it uses a really stupid logic to determine GZip chunks when parsing decompressed file. It just looks for fixed 10-byte GZip header and consider it as a strat of new compressed "member". Such behaviour could lead to decompression error in case when compressed member contains this 10-bytes sequence. I've considered this as a tradeoff for the sake to concentrate on multithreaded logic.

# GZIP parallel compression/decompression
## Main idea
Since GZIP file is a series of compressed data sets, it is possible to split file to be compressed to blocks, then compress them independently in the separate threads using System.IO.Compression.GzipStream (which produces the correct GZIP data set) and finally write resulting compressed data into destination file. The reverse process consists of reading those data sets from the compressed file according to GZIP data fromat specified in RFC1952, decompress them independentely and compose the destination file.
## The importance of maintaining original order of blocks
It is obvious that both compression and decompression processes must retain the order of blocks read from source file when composing the result.
## Reading of compressed file
This naive implementation threats the two-bytes sequence 0x1f 0x8b as a start of compressed set.

# The architecture of the application
## Main idea
Both compression and decompression could be broken down into the following processes:
- Reading source file
- Compression
- Writing destination file

Reading and writing files could be done only sequentally, while compression could be done in parallel.
There is an established practice to organize a communcation between one reader/writer thread and multiple processing threads using the Producer-Consumer pattern, so it is a basis of the application's architecture.
## Producer-consumer chains
### Reader -> processors
The reader thread reads the input file sequentally and produces the blocks of data to compress/decompress. A number of processor threads consume them to perform compression or decompression.

Since the reader could produce the data very fast, which could lead to RAM overrun, the capacity of this chain should be restricted according to the number of processing threads.

Each block of data generated by reader must contain its sequence number in the input file in order to retain the order in the output file.
### Processors -> Writer
Processing threads produce data in random order, but writer thread must consume in the order from inpput file. Hence this chain must always provide the blocks of data in the proper order. It should block writer if the following block is not ready yet, even if there are next blocks available.

Because it is too hard (personally for me) to implement the proper loginc in the single producer-consumer queue, I've decided to implement one producer-consumer queue for each processing thread producing data. Each this queue provide the data in the order of ascending ids. Writer itself stores the last written id and examines those queues for the next.

This approach with combining multiple producers-consumers automatically provides synchronization for writing blocks without implementing additional synchronization logic.

### Reader - writer
This producer-consumer chain is very simple and is intended to pass the last id read from the input file to writer. Writer ends its job when the last written id equals to this id passed from reader.
### Total
There are 2+N producer-consumer chains, where N is the number of processing threads. In this scheme processing threads are intended to be both producers and consumers.

## Thread management
Overall count of threads - 3+N, where N - count of logical CPUs:
- Main app thread
- File reader thread
- N compressor threads
- File writer thread
Threads are started once and no restart or pooling needed due to nature of producer-consumer pattern.

# Solution structure
There are the following projects in the solution:
- GZipTest.App - main application project, builds console app.
- GZipTest.RegressionTests - regression tests which use the real app built. Some hard-code for app path, but it is a study project so I've not set up proper CI.
- GZipTest.Tests - unit tests.